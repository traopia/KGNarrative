[
    {
        "test_loss": 0.15330421924591064,
        "test_rouge1": 0.6704198768260525,
        "test_rouge2": 0.41819944172814405,
        "test_rougeL": 0.4640008332641641,
        "test_rougeLsum": 0.4644037580958007,
        "test_runtime": 110.5802,
        "test_samples_per_second": 3.02,
        "test_steps_per_second": 0.76
    },
    {
        "bleu": 0.3126856844674224,
        "precisions": [
            0.6340241559154542,
            0.37814299761202413,
            0.24230437284234752,
            0.164554163596168
        ],
        "brevity_penalty": 1.0,
        "length_ratio": 1.0635720020436465,
        "translation_length": 14572,
        "reference_length": 13701
    },
    {
        "google_bleu": 0.3303011497256337
    },
    {
        "meteor": 0.5173637676579932
    },
    {
        "Bert_Score": {
            "precision": 0.8848333490822844,
            "recall": 0.9004843854261729,
            "f1": 0.8921779740356399
        }
    },
    {
        "bleurt_score": 0.689389670680383
    },
    {
        "PARENT": {
            "precision": 0.3995130619424498,
            "recall": 0.4480529913425726,
            "f_score": 0.39618797426237634
        }
    },
    {
        "time(s)": 694.826952457428
    },
    {
        "gpu": 11492
    }
][
    {
        "typeKG": "Types_KG"
    },
    {
        "hyper_Params": [
            0.0001,
            1,
            3
        ]
    },
    {
        "test_loss": 1.0926238298416138,
        "test_rouge1": 0.22260070369210244,
        "test_rouge2": 0.022393753540619663,
        "test_rougeL": 0.1946004165231293,
        "test_rougeLsum": 0.19438442960152874,
        "test_runtime": 91.1325,
        "test_samples_per_second": 3.665,
        "test_steps_per_second": 0.922
    },
    {
        "bleu": 0.0,
        "precisions": [
            0.28565790512274053,
            0.02245642480745845,
            0.0004999583368052662,
            0.0
        ],
        "brevity_penalty": 0.9217707978673908,
        "length_ratio": 0.9246770308736588,
        "translation_length": 12669,
        "reference_length": 13701
    },
    {
        "google_bleu": 0.06724339973805749
    },
    {
        "meteor": 0.09348826634358957
    },
    {
        "Bert_Score": {
            "precision": 0.6090983768066246,
            "recall": 0.5817117178868391,
            "f1": 0.5948973710308532
        }
    },
    {
        "bleurt_score": 0.31348171181039897
    },
    {
        "time(s)": 753.7802114486694
    },
    {
        "gpu": 11492
    }
]