[
    {
        "typeKG": "Instances_list"
    },
    {
        "hyper_Params": [
            0.0001,
            1,
            3
        ]
    },
    {
        "test_loss": 1.6997733116149902,
        "test_rouge1": 0.42438700969885923,
        "test_rouge2": 0.1127144124625394,
        "test_rougeL": 0.16606668992979703,
        "test_rougeLsum": 0.16608767502002672,
        "test_runtime": 294.1065,
        "test_samples_per_second": 0.289,
        "test_steps_per_second": 0.075
    },
    {
        "bleu": 0.0523460532905648,
        "precisions": [
            0.476710753306498,
            0.1255420760827798,
            0.02962942584390217,
            0.010478711669975733
        ],
        "brevity_penalty": 0.7972877615033351,
        "length_ratio": 0.8153018396142169,
        "translation_length": 36519,
        "reference_length": 44792
    },
    {
        "google_bleu": 0.1181511164877262
    },
    {
        "meteor": 0.22090968443003128
    },
    {
        "Bert_Score": {
            "precision": 0.7843763176132651,
            "recall": 0.7804371272816377,
            "f1": 0.781995610629811
        }
    },
    {
        "bleurt_score": 0.3461488310028525
    },
    {
        "time(s)": 760.9236161708832
    },
    {
        "gpu": 14278
    }
][
    {
        "typeKG": "Instances_list"
    },
    {
        "hyper_Params": [
            0.0001,
            1,
            3
        ]
    },
    {
        "test_loss": 7.9551873207092285,
        "test_rouge1": 0.12596935444629287,
        "test_rouge2": 0.00042447839076937703,
        "test_rougeL": 0.043481983996201734,
        "test_rougeLsum": 0.04348766239635516,
        "test_runtime": 586.8138,
        "test_samples_per_second": 0.145,
        "test_steps_per_second": 0.037
    },
    {
        "bleu": 0.0,
        "precisions": [
            0.10204904808002581,
            0.0010780799396275234,
            0.0,
            0.0
        ],
        "brevity_penalty": 0.8150754757992162,
        "length_ratio": 0.830237542418289,
        "translation_length": 37188,
        "reference_length": 44792
    },
    {
        "google_bleu": 0.019554952731574492
    },
    {
        "meteor": 0.08885521819038383
    },
    {
        "Bert_Score": {
            "precision": 0.605105550850139,
            "recall": 0.6335800430353951,
            "f1": 0.6189355191062479
        }
    },
    {
        "bleurt_score": 0.09574123051236658
    },
    {
        "time(s)": 693.20822930336
    },
    {
        "gpu": 23994
    }
]