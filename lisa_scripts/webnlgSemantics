#!/bin/bash
#SBATCH --job-name=webnlg_final_semantic
#SBATCH --time=20:00:00
#SBATCH -N 1
## #SBATCH --ntasks-per-node=1
#SBATCH --partition=gpu_titanrtx_shared
#SBATCH --gpus-per-task=4
#SBATCH --output=webnlg_final_semantic_out
## in the list above, the partition name depends on where you are running your job. 
## On DAS5 the default would be `defq` on Lisa the default would be `gpu` or `gpu_shared`
## Typing `sinfo` on the server command line gives a column called PARTITION.  There, one can find the name of a specific node, the state (down, alloc, idle etc), the availability and how long is the ti$


# Load GPU drivers
## For Lisa modules are usually not needed, so remove the previous 2 lines. https://userinfo.surfsara.nl/systems/shared/modules 



# This loads the anaconda virtual environment with our packages
###export CUDA_VISIBLE_DEVICES=0,1,2,3


conda activate kg2Narrative

# Base directory for the experiment. This should be your outer folder, actually where this script is 
cd /home/tliberatore/KGNarrative


#semantics
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'Instances_KG' bart-large --learning_rate 0.0001 --batch 1 --epochs 3
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'Types_KG' bart-large --learning_rate 0.0001 --batch 1 --epochs 3
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'Instances_list' bart-large --learning_rate 0.0001 --batch 1 --epochs 3
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'Subclasses_KG' bart-large --learning_rate 0.0001 --batch 1 --epochs 3
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'multi_Subclasses_KG' bart-large --learning_rate 0.0001 --batch 1 --epochs 3

#reification
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'entities_list' bart-large --learning_rate 0.0001 --batch 1 --epochs 3
python3 KGNarrative/script4trainingLLM/finetunemodel_webnlg.py KGNarrative/Datasets/WebNLG/4experiment 'semantic_of_news' bart-large --learning_rate 0.0001 --batch 1 --epochs 3




