{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subclasses(subclasses_list):\n",
    "    #print(subclasses_list)\n",
    "    data=subclasses_list.split(\" | \")\n",
    "    clean=[]\n",
    "    if len(data)>1:\n",
    "        for tr in data:\n",
    "            spo = tr.split(\" - \")\n",
    "            if spo[0] == spo[2] or spo[2]=='Thing':\n",
    "                continue\n",
    "            clean.append((\" - \").join(spo))\n",
    "        return (\" | \").join(clean)\n",
    "    else: return subclasses_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/DWIE/DWIE_cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns1():\n",
    "\n",
    "    splits=['train','dev','test']\n",
    "    splits=['validation']\n",
    "    for split in splits:\n",
    "\n",
    "        with open(path+f'{split}_cleaned_mined.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        \"\"\"with open(f'/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/WebNLG/57_triples/Multiple_Classes/{split}_57_MultipleClass.json', 'r') as f:\n",
    "            data_subclass = json.load(f)\n",
    "        data_subclass=data_subclass[1:] #THIS is because the first line got lost somehow\"\"\"\n",
    "\n",
    "\n",
    "        instances = ['Instances_KG']\n",
    "\n",
    "        typeKG = ['Instances_KG', 'Types_KG']\n",
    "\n",
    "        subClassKG = ['Instances_KG', 'Types_KG','Subclasses_KG']\n",
    "\n",
    "        for d,d_s in zip(data,data_subclass):\n",
    "            \n",
    "            #print(\"[CORE] \"+ d['core_description'] +\" [TRIPLES]\")\n",
    "\n",
    "            #MERGE CGRAPHS AND ADD CORE\n",
    "\n",
    "            merged_types = \"[CORE] \"+ d['core_description'] +\" [TRIPLES] \"+' | '.join([d[k] for k in typeKG])\n",
    "\n",
    "            merged_subClasse = \"[CORE] \"+d['core_description'] + \" [TRIPLES] \" + ' | '.join([d[k] for k in subClassKG])\n",
    "\n",
    "            merged_instances = \"[CORE] \"+d['core_description'] + \" [TRIPLES] \" + ' | '.join([d[k] for k in instances])\n",
    "\n",
    "            d_s['Subclasses_KG']=clean_subclasses(d_s['Subclasses_KG'])\n",
    "            d['multi_Subclasses_KG'] = \"[CORE] \"+ d['core_description'] +\" [TRIPLES] \"+' | '.join([d[k] for k in typeKG]) + d_s['Subclasses_KG']\n",
    "\n",
    "\n",
    "            d['Types_KG'] = merged_types.replace('\"\"\"','')\n",
    "            d['Subclasses_KG'] = merged_subClasse.replace('\"\"\"','')\n",
    "            d['Instances_KG'] = merged_instances.replace('\"\"\"','')\n",
    "\n",
    "            #ADD CORE TO ENTITIES LIST AND SEMANTIC OF NEWS\n",
    "\n",
    "            d['Instances_list'] = (\"[CORE] \"+d['core_description']+ \" [ENTITIES] \" + \" | \".join([x.strip() for x in d['Instances_list']])).replace('\"\"\"','')\n",
    "            d['entities_list'] = (\"[CORE] \"+d['core_description']+ \" [ENTITIES] \" + \" | \".join([x.strip() for x in d['entities_list']])).replace('\"\"\"','')\n",
    "\n",
    "            d['semantic_of_news'] = (\"[CORE] \"+d['core_description']+ \" [TRIPLES] \" + d['semantic_of_news']).replace('\"\"\"','')\n",
    "\n",
    "\n",
    "            \n",
    "        # Save the updated list of dictionaries back to the same JSON file\n",
    "        with open(f'/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/WebNLG/4experiment/full_{split}.json', 'w') as f:\n",
    "                    json.dump(data,f,indent=4,ensure_ascii = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/DWIE/DWIE_cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=['dev','test','train']\n",
    "\n",
    "\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "    new_ds=[]\n",
    "\n",
    "\n",
    "    with open(path+f'{split}_cleaned_mined.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    #data=data[10:11]\n",
    "    \n",
    "\n",
    "    \"\"\"with open(f'/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/WebNLG/57_triples/Multiple_Classes/{split}_57_MultipleClass.json', 'r') as f:\n",
    "        data_subclass = json.load(f)\n",
    "    data_subclass=data_subclass[1:] #THIS is because the first line got lost somehow\"\"\"\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "      \n",
    "        \n",
    "        clean={}\n",
    "\n",
    "        clean['story'] = d['story']\n",
    "\n",
    "        clean['Instances_KG']  = \"[CORE] \"+ d['core description'] +\" [TRIPLES] \"+ d['Instances']     \n",
    "\n",
    "        clean['Types_KG'] = \"[CORE] \"+d['core description'] + \" [TRIPLES] \" + d['Instances_Types']\n",
    "\n",
    "        clean['Subclasses_KG']  = \"[CORE] \"+d['core description'] + \" [TRIPLES] \" + d['Instances_Types_subClasses']\n",
    "\n",
    "        clean['entities_list']  = (\"[CORE] \"+d['core description']+ \" [ENTITIES] \" + \" | \".join([x.strip() for x in d['entities_list']])).replace('\"\"\"','')\n",
    "\n",
    "        clean['core_description'] = d['core description']\n",
    "\n",
    "        clean['semantic_of_news'] = (\"[CORE] \"+d['core description']+ \" [TRIPLES] \" + d['semantic_of_news']).replace('\"\"\"','')\n",
    "\n",
    "        clean['Instances_list'] = (\"[CORE] \"+d['core description']+ \" [ENTITIES] \" + \" | \".join([x.strip() for x in d['Instances_list']])).replace('\"\"\"','')\n",
    "\n",
    "        if len(word_tokenize(clean['story']))>1024 or len(word_tokenize(clean['Subclasses_KG']))>4096:\n",
    "              continue\n",
    "        else:\n",
    "            new_ds.append(clean)\n",
    "\n",
    "    # Save the updated list of dictionaries back to the same JSON file\n",
    "    with open(f'/home/gabhoo/research/kg2Narrative/KGNarrative2/Datasets/DWIE/4experiment/full_{split}.json', 'w') as f:\n",
    "                json.dump(new_ds,f,indent=4,ensure_ascii = False)\n",
    "\n",
    "     \n",
    "#new_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptasks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
